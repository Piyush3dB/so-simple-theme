---
layout: post
title: Maximum entropy expression
excerpt: Most academic literature write this shorthand
categories: blog
tags: [Information Theory]
comments: true
---

Let $$X = \{x_i\}_{i=0}^{N-1}$$ be a set of $$N$$ equi-probable events.  The probability of event $$x_i$$ is then $$P(X=x_i)=\frac{1}{N}$$ for $$i \in [0,N-1]$$.  Most academic literature will state entropy of X as 

$$
H(X) = log_2(N)
$$

which can be confusing since $$H(X)$$ is formally defined as

$$
H(X) = -\sum_{i=0}^{N-1}P(x_i)log_2(P(x_i))
$$

With some manipulation we can show the above expression simplifies to the compact form

$$
\begin{eqnarray}
H(X) & = & -\sum_{i=0}^{N-1}P(x_i)log_2(P(x_i)) \\
& = & -\sum_{i=0}^{N-1}\frac{1}{N}log_2(\frac{1}{N}) \\
& = & -\frac{N}{N}log_2(\frac{1}{N}) \\
& = & log_2(N) \\
\end{eqnarray}
$$

Therefore the entropy of a discrete uniform random variable is the $$log_2$$ of the total number of possible events.  Knowing this some academic literature starts to make sense.
