---
layout: post
title: Formalizing intuitive ideas about information
excerpt: What is relevant information in the Information Theoretic sense?
categories: blog
tags: [information theory, machine learning]
comments: true
---

Providing a quantitative notion of relevant or meaningful information is a fundamental problem in formalizing intuitive ideas about information.

Shannon's original formulation of [Information Theory](https://en.wikipedia.org/wiki/Information_theory) was motivated by the problem of transmitting information rather than judging its value to the recipient.  An unintended consequence of this is information theory has often been viewed as being strictly a theory of communication, and this view has become so accepted in the mainstream that many consider statistical and information theoretic principles as almost irrelevant for the question of meaning of information.

Yet certain elements of information theory, in particular lossy source compression, provide a natural quantitative approach to the question of "relevant information".

Lossless compression is entropy preserving, but any compression beyond the entropy of a signal means some components of it cannot exactly be reconstructed.  If a signal has much lower information entropy relative to its energy then it means it is possible to compress without losing any relevant information about the meaning it conveys.

Exact reconstruction in the Signal Processing sense isn't quite the same as exact reconstruction in the Information Theoretic sense because the theory allows for entropy preservation even under lossy compression schemes.

Of course the problem of extracting a relevant summary of data, a compressed description that captures only the meaningful information, is not well-posed without a suitable definition of relevance, but this is a different issue all together.

The standard analysis of lossy compression is [Rate Distortion Theory](https://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory), which provides a trade-off between the bit-rate, or signal representation size, and the average distortion of the reconstructed signal. Rate distortion theory determines the level of inevitable expected distortion, $D$, given the desired information rate, $R$, in terms of the rate distortion function $R(D)$.

<figure class="half">
  <a href="http://www.data-compression.com/ratedist.gif"> <img src="http://www.data-compression.com/ratedist.gif" alt="image"></a>
  <figcaption> Typical rate distortion curve. </figcaption>
</figure>


The main problem with rate distortion theory is in the need to specify the distortion function first, which in turn determines the relevant features of the signal. Those features, however, are often not explicitly known and an arbitrary choice of the distortion function is in fact an arbitrary feature selection.  In a broader sense, discovery of the distortion function is the ultimate goal of the machine learning problem.  Rate distortion theory does not provide a full answer to this problem since the choice of the distortion function, which determines the relevant features, is not part of the theory. It does, however, provide a theoretical starting point when thinking about "relevant information".
