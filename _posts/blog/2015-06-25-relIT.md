---
layout: post
title: Formalizing intuitive ideas about information
excerpt: What is relevant information in the Information Theoretic sense?
categories: blog
tags: [information theory, machine learning]
comments: true
---

Providing a quantitative notion of relevant or meaningful information is a fundamental problem in formalizing intuitive ideas about information.

Shannon's original formulation of [Information Theory](https://en.wikipedia.org/wiki/Information_theory) was motivated by the problem of transmitting information rather than judging its value to the recipient.  An unintended consequence of this is information theory has often been viewed as being strictly a theory of communication, and this view has become so accepted in the mainstream that many consider statistical and information theoretic principles as almost irrelevant for the question of meaning of information.

Yet certain elements of information theory, in particular lossy source compression, provide a natural quantitative approach to the question of "relevant information".

Lossless compression is entropy preserving, but any compression beyond the entropy of a signal means some components of it cannot exactly be reconstructed.  If a signal has much lower information entropy relative to its energy then it means it is possible to compress without losing any relevant information about the meaning it conveys.

Exact reconstruction in the Signal Processing sense isn't quite the same as exact reconstruction in the Information Theoretic sense because the theory allows for entropy preservation even under lossy compression schemes.

Of course the problem of extracting a relevant summary of data, a compressed description that captures only the meaningful information, is not well-posed without a suitable definition of relevance, but this is a different issue all together.

The standard analysis of lossy compression is [Rate Distortion Theory](https://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory), which provides a trade-off between the bit-rate, or signal representation size, and the average distortion of the reconstructed signal. Rate distortion theory determines the level of inevitable expected distortion, $D$, given the desired information rate, $R$, in terms of the rate distortion function $R(D)$.

<figure class="half">
	<a href="http://www.data-compression.com/ratedist.gif"> <img src="http://www.data-compression.com/ratedist.gif" alt="image"></a>
	<figcaption> Typical rate distortion curve. </figcaption>
</figure>


The main problem with rate distortion theory is in the need to specify the distortion function first, which in turn determines the relevant features of the signal. Those features, however, are often not explicitly known and an arbitrary choice of the distortion function is in fact an arbitrary feature selection.  In a broader sense, discovery of the distortion function is the ultimate goal of the machine learning problem.  Rate distortion theory does not provide a full answer to this problem since the choice of the distortion function, which determines the relevant features, is not part of the theory. It does, however, provide a theoretical starting point when thinking about "relevant information".




--------------------

Correntropy is a nonlinear and local similarity measure directly related to the probability of how similar two random variables are in a neighborhood of the joint space controlled by the PDF estimator kernel bandwidth.

Correntropy has received increasing attention in domains of machine learning and signal processing as demonstrated by one of many papers recently submitted to the IEEE Transactions on Signal Processing:

> ... In this work, we propose a generalized correntropy that adopts the generalized Gaussian density (GGD) function as the kernel (not necessarily a Mercer kernel), and present some important properties. We further propose the generalized maximum correntropy criterion (GMCC), and apply it to adaptive filtering. An adaptive algorithm, called the GMCC algorithm, is derived, and the mean square convergence performance is studied. We show that the proposed algorithm is very stable and can achieve zero probability of divergence (POD). Simulation results confirm the theoretical expectations and demonstrate the desirable performance of the new algorithm. 

Link: [Generalized Correntropy for Robust Adaptive Filtering, April 2015](http://arxiv.org/abs/1504.02931)

Key points to take away from the paper:




* The mean square error (MSE) criterion and variants of the least mean square (LMS) algorithm such as normalized LMS (NLMS) and variable step-size LMS (VSSLMS)  is widely used as a cost function since it has attractive features, such as smoothness, convexity, mathematical tractability, low computational burden and optimality under Gaussian assumption.

* The MSE criterion is sub-optimal when the Gaussain assumption does not hold for the signals being adapted, and an alternate criterion is sought.

* In recent years, the maximum correntropy criterion (MCC) has been successfully used in robust adaptive filtering, wherein the filter weights are adapted such that the correntropy between the desired signal and filter output is maximized.

* Since correntropy is insensitive to outliers especially with a small kernel bandwidth,  it is naturally a robust adaptation cost in presence of heavy-tailed impulsive noises.

* The optimal solution of GMCC filtering is in form similar to the well-known Wiener solution, except that the autocorrelation matrix and cross-correlation vector are weighted by an error nonlinearity. If the signals involved are zero-mean Gaussian, the optimal solution will equal to the Wiener solution.

* **Weight adaptation applies to the training of adaptive filters or neural networks, hence the relevance to machine learning.**