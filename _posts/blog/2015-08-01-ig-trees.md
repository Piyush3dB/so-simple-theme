---
layout: post
title: Information Gain in Decision Trees
excerpt: In this post I write about Information Entropy and how it applies to Decision Tree construction. 
categories: blog
tags: [machine learning]
comments: true
---


In this post I write about Information Entropy and how it applies to Decision Tree construction.  We also look at why it makes sense for popular machine learning algorithms such as [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) and [C4.5](https://en.wikipedia.org/wiki/ID3_algorithm) to use this metric as  measure for cost.

#### Many Interpretations of Mutual Information

Let $$I(S;A)$$ denote the Mutual Information between a sample set $$S$$ and a set of attributes $$A$$ by which set $$S$$ can be partitioned.  Then $$I(S;A)$$ can be interpreted in many ways:

* **(Uncertainty Reduction)** The amount by which uncertainty in the samples $$S$$ is reduced by knowing attributes $$A$$, or

* **(Information Transfer)** The amount of information about the the samples $$S$$ conveyed by the attributes $$A$$, or

* **(Information Contribution)** The amount of information shared by samples $$S$$ and attributes $$A$$, or

* **(Information Gain)** The amount of Information about samples $$S$$ gained by knowing attributes $$A$$, or

Information Gain is a popular cost function for building Decision Trees in Machine Learning.  The basic idea is partitioning a sample set $$S$$ into a number of attributes $$A$$ is so as to maximise the Mutual Information between the un-partitioned and partitioned sets.

####Example

Lets look at the example referred to in this slide:


<figure >
  <a href="http://homes.cs.washington.edu/~shapiro/EE596/notes/InfoGain.pdf"> <img src="/images/InfoGainSlide.png" alt="image"></a>
  <figcaption> Calculating Information Gain. </figcaption>
</figure>


Here we have $$s \in S = \{+,\bigcirc\}$$ denoting the types to be partitioned according to attributes $$a \in A =\{T,B\} $$ denoting Top partition and Bottom groups.

1. For the unpartitioned set we have

   * A total of $$30$$ elements.
   * $$16$$ circles and $$14$$ crosses giving $$ P(\bigcirc) = \frac{16}{30} $$ and $$ P(+) = \frac{14}{30} $$.
   * $$17$$ shapes with attribute Top and $$13$$ shapes with attribute Bottom giving $$ P(T) = \frac{17}{30} $$ and $$ P(B) = \frac{13}{30} $$.

2. For the Top Partition we have $$ P(+ \mid T) = \frac{13}{17} $$ and $$ P(\bigcirc \mid T) = \frac{4}{17} $$

3. For the Bottom Partition we have $$ P(+ \mid B) = \frac{1}{13} $$ and $$ P(\bigcirc \mid B) = \frac{12}{13} $$

The Mutual Information cost metric for this is defined as

$$
I(S;A) = H(S) - H(S|A)
$$

Before partitioning the entropy is

$$H(S) = -\sum_{s \in \{\bigcirc, + \} }^{} P(s)\log_2 P(s) $$ 

but after partitioning the entropy is reduced to 

$$
\begin{eqnarray}
H(S \mid A) &=& \mathbb{E}_{a \in A}[H(S \mid A)] \\
&=&-\sum_{a \in A} P(a)  \sum_{s \in S}P(s \mid a) \log_2P(s \mid a)  \\
&=&-\sum_{a \in \{T,B \}} P(a)\left(  \sum_{s \in \{+,\bigcirc \}}P(s \mid a) \log_2P(s \mid a) \right) \\
&=&- P(T)  P(+ \mid T) \log_2P(+ \mid T)  \\
& & - P(T)  P(\bigcirc \mid T) \log_2P(\bigcirc \mid T)  \\
& &- P(B)  P(+ \mid B) \log_2P(+ \mid B)  \\
& & - P(B)  P(\bigcirc \mid B) \log_2P(\bigcirc \mid B) 
\end{eqnarray}
$$

The aim during decision tree construction is to preserve as much information entropy as possible as the data is split at subsequent steps, and the chosen way of measuring the amount to information preserved is via mutual information $$I(S;A)$$.  Since $$H(S \mid A)$$ tells us the amount of information about $$S$$ that is not conveyed due to the attribute $$A$$, minimising this ensures information $$I(S;A)$$ about the samples conveyed by the attribute is maximised.  

What this means in terms of decision tree construction is that each successor is split in a way that partitions end up with the most number of similar elements.  
