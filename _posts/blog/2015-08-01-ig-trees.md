---
layout: post
title: Information Gain in Decision Trees
excerpt: 
categories: blog
tags: [visualisation]
comments: true
---

Interpretations of Mutual Information


Let $$I(S;A)$$ denote the Mutual Information between a sample set $$S$$ and a set of attributes $$A$$ which partition set $$S$$.  $$I(S;A)$$ can be interpreted as:

* The amount of information about the the samples $$S$$ conveyed by the attributes $$A$$.

* The amount of information shared by samples $$S$$ and attributes $$A$$.

* The amount by which uncertainty in the samples $$S$$ is reduced by knowing attributes $$A$$.

* The amount of Information gained about samples $$S$$ by knowing the attributes $$A$$.

Information Gain is a popular cost function for building Decision Trees in Machine Learning.  The basic idea is a partitioning of a sample set $$S$$ into possible attributes is so as to maximise the Mutual Information between the un-partitioned and partitioned sets.

The Mutual Information cost metric for this is defined as

$$
I(S;A) = H(S) - H(S|A)
$$

where

*  $$H(S) = -\sum_{s \in S}^{} P(s)\log_2 P(s) $$ 

is the entropy of the set to be partitioned, and

*  
$$
\begin{eqnarray}
H(S \mid A) &=& \mathbb{E}_{a \in A}[H(S \mid A)] \\
&=&-\sum_{a \in A} P(a) \sum_{s \in S}P(s \mid a) \log_2P(s \mid a)
\end{eqnarray}
$$

is the conditional entropy of set S given the partitions according to attributes A.  
This tells us the amount of information about S that is not conveyed by A.

Notice that $$I(S;A)$$ is bounded to the range $$[0, H(S)]$$ is maximised when $$H(S \mid A)$$ is minimised.  

There are numerous examples showing how to apply this procedure.  For illustrating purposes we refer the the following example slide:

<figure >
  <a href="/images/InfoGainSlide.png"> <img src="/images/InfoGainSlide.png" alt="image"></a>
  <figcaption> j. </figcaption>
</figure>

Define 
$$a \in A =\{T,B\} $$ denoting all possible attributes, in this case a Top partition and Bottom partition
$$s \in S = \{+,\bigcirc\}$$ denoting the types of shapes in the partition

$$ P(T) = \frac{17}{30} $$ and $$ P(B) = \frac{13}{30} $$

$$ P(+ \mid T) = \frac{}{} $$

$$ P(\bigcirc \mid T) = \frac{}{} $$


$$ P(+ \mid B) = \frac{}{} $$ and $$ P(\bigcirc \mid B) = \frac{}{} $$


The entropy of the unpartitioned set is straightforward to compute. Perhaps a bit more involving is the conditional entropy:


$$
\begin{eqnarray}
H(S \mid A) &=&-\sum_{a \in A} P(a) \sum_{s \in S}P(s \mid a) \log_2P(s \mid a) \\
&=&-\sum_{a \in \{T,B \}} P(a) \sum_{s \in \{+,\bigcirc \}}P(s \mid a) \log_2P(s \mid a) \\
&=&- P(T)  P(+ \mid T) \log_2P(+ \mid T)  \\
& & - P(T)  P(\bigcirc \mid T) \log_2P(\bigcirc \mid T)  \\
& &- P(B)  P(+ \mid B) \log_2P(+ \mid B)  \\
& & - P(B)  P(\bigcirc \mid B) \log_2P(\bigcirc \mid B)  \\
&=& \frac{17}{30} \left(-\frac{13}{17}\log_2\frac{13}{17} -\frac{4}{17}\log_2\frac{4}{17} \right)\\
& & \frac{13}{30}\left( -\frac{1}{13}\log_2\frac{1}{13} -\frac{12}{13}\log_2\frac{12}{13} \right)\\
&=& 0.615
\end{eqnarray}
$$

