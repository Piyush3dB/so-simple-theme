---
layout: post
title: "What exactly is a Bayesian filter?"
excerpt: "Information Theoretic perspective on Bayesian filtering"
categories: articles
tags: [signal processing, information theory, machine learning]
comments: true
share: true
---

A Bayesian filter describes a procedure for applying Bayes' rule to recursively estimate random variables, probabilities or parameters using priors and new information.  Infact a number of statistical estimation algorithms are derived as special cases of the generalised Bayesian filter, for instance the Kalman filter and and Monte Carlo.  The algorithmic setup involves estimating an unobservable random variable (hidden state) using observable measurements that convey some information about the hidden state.  The Bayesian formulation enforces certain assumptions and allows for recursive estimation of the hidden state by incorporating prior knowledge with new measurements as they become available.

The derivation of a recursive Bayesian filter is well covered in a number of texts, but most lack an intuitive explanation of the procedure.  In this post we shall go through the derivation and in the end apply an Information Theoretic simplification to the expression to provide a different take on what exactly what a Bayesian filter achieves.


Start with Bayes' rule:

$$
P(x_n \mid Y_n) = \frac{ P(x_n) P(Y_n \mid x_n)} {P(Y_n)}
$$

where

*  $$n$$ is a temporal index
* $$x_n$$ is the hidden internal state at different time indices $$n$$ to be inferred from observations $$Y_n$$
* $$Y_n = \{ y_1, y_2, ..., y_n \} $$ are a set of observations that provide information about hidden state $$x_n$$
* $$p(x_n)$$ is the hidden state prior
* $$P(Y_n \mid x_n)$$ is the likelihood of the measurements given the hidden state

It is worth highlighting what the term filter means in this context.  In Signal Processing a filter refers to a convolution kernel, but in estimation theory the term has a different meaning.

Recursive Bayesian estimation can take 3 different forms depending on what version of the hidden state is being estimated from available observations:

1.  **Bayesian smoothing**: estimating the past hidden state given the present and past observations, 
2. **Bayesian filtering**: estimating the current hidden state given the present and past observations, and
3.  **Bayesian prediction**: estimating a probable future hidden state given the present and the past observations. 

#### Derivation

Deriving a Bayesian filter requires finding an expression for how the posterior $$P(x_n \mid Y_n)$$ relates to the prior $$P(x_n \mid Y_{n-1})$$ and the likelihood.  In this way the posterior  $$P(x_n \mid Y_{n-1})$$ from a past step $$n-1$$  becomes the prior for the posterior  $$P(x_n \mid Y_{n})$$ of the current step $$n$$, and this is where the recursive part of of the algorithm comes from.

Using Bayes' rule as a starting point we proceed by first splitting the observations $$Y_n$$ into two sets:

1.  $$y_n$$ - the current observation
2.  $$Y_{n-1} = \{ y_{n-1}, y_{n-2},...,y_1,y_0 \}$$ - set of all past observations

Bayes' rule with this substitution becomes:

$$
\overbrace{P(x_n \mid y_n, Y_{n-1}) = P(x_n \mid Y_{n}) }^{posterior} =  \frac{ \overbrace{ P(x_n) } ^{prior} \overbrace{ P(y_n,Y_{n-1}  \mid x_n) }^{likelihood} } { \underbrace{ P(y_n, Y_{n-1}) } _{normalisation}}
$$

The RHS of the expression needs to be expanded to obtain the prior $$P(x_n \mid Y_{n-1})$$ as required for a recursive formula.

Using the product rule the following expansions are obtained:

*   For the likelihood term in the numerator 
$$ 
P(y_n, Y_{n-1} \mid x_n) = P(Y_{n-1} \mid x_n) P(y_n \mid x_n, Y_{n-1})  
$$

*   For the normalisation term in the denominator
$$
P(y_n, Y_{n-1}) = P(y_n \mid Y_{n-1}) P(Y_{n-1})
$$

and by substitution we get

$$
P(x_n \mid Y_n) = \frac{ P(x_n) P(y_n \mid Y_{n-1}, x_n) P(Y_{n-1} \mid x_n)} {P(y_n \mid Y_{n-1}) P(Y_{n-1})}
$$

We apply Bayes' rule again to rewrite part of the above equation as

$$
\frac{P(x_n) P(Y_{n-1} \mid x_n )}{P(Y_{n-1})} = P(x_n \mid Y_{n-1})
$$

and the recursion emerges as

$$
P(x_n \mid Y_n) = \frac{ P(y_n \mid Y_{n-1}, x_n) } {P(y_n \mid Y_{n-1}) } P(x_n \mid Y_{n-1})
$$

A further simplfication can be made by observing $$P(y_n \mid Y_{n-1}, x_n) = P(y_n \mid x_n)$$ because $$y_n$$ and the set of past measurements $$Y_{n-1}$$ describe the same random variable with non-varying statistical properties and can be assumed to be independent of each other.

The expression for a recursive Bayesian filter is then

$$
\overbrace{P(x_n \mid Y_n)}^{current \space posterior} = \frac{ \overbrace{ P(y_n \mid  x_n)}^{likelihood} } {\underbrace{P(y_n \mid Y_{n-1}) }_{normalisation}  } \overbrace{P(x_n \mid Y_{n-1})}^{previous \space posterior}
$$

Thus the filter specifies that the current improved estimate is a proportion of the previous estimate.

####Information Theoretic Analysis

A different way of understanding the dynamics of the filter is to first apply the standard definition of information entropy $$H(*) = -\log_2 P(*)$$ and re-arrange the emerging terms:

$$
\begin{eqnarray}
-log_2 P(x_n \mid Y_n) &=&  -log_2 \left( \frac{ P(y_n \mid  x_n) } {P(y_n \mid Y_{n-1}) } P(x_n \mid Y_{n-1}) \right) \\
&=&  H(y_n \mid  x_n) - H(y_n \mid Y_{n-1}) +H(x_n \mid Y_{n-1}) \\
&=& H(x_n \mid Y_{n-1}) - \left (  H(y_n \mid Y_{n-1}) - H(y_n \mid  x_n)  \right ) \\
\underbrace{H(x_n \mid Y_n) }_{current \space uncertainty}&=& \underbrace{H(x_n \mid Y_{n-1})}_{previous \space uncertainty} -  \underbrace{I(y_n \mid Y_{n-1} ; y_n \mid  x_n)}_{evidential \space uncertainty}  \\
\end{eqnarray}
$$

We see there are three components to the alternate form:

1.   $$H(x_n \mid Y_n)$$ - **Posterior** uncertainty in the hidden state on observation of all measurements.
2.   $$H(x_n \mid Y_{n-1})$$ - **Prior** uncertainty in the hidden state on observation of all past measurements.
3.   $$I(y_n \mid Y_{n-1} ; y_n \mid  x_n)$$ - **Information** about the hidden state conveyed by all available measurements.  This Mutual Information.

Thus the purpose of a recursive Bayesian filter is to reduce uncertainty in the estimate of the hidden state by incorporating information about the hidden state conveyed by the current observation.



