---
layout: post
title: "Information Theory and Machine Learning"
excerpt: "Information Theory now lies at the heart of ML research"
categories: articles
tags: [signal processing, information theory, machine learning]
comments: true
share: true
---


Information Theory and Learning Theory, historically treated as separate subjects, now lie at the heart of some of the most exciting areas in Machine Learning and Artificial Intelligence research.  Information Theory was developed by Claude Shannon around 1948 to find the fundamental limits on signal processing operations in communication systems.  Learning Theory is a subfield of Computer Science that is concerned with learning and making inferences from data.

<figure>
	<a href="/images/it.png"><img src="/images/it.png" alt="image" align="middle"></a>
	<figcaption><a href="/images/it.png" title=" "> </a> </figcaption>
</figure>

One of the key goals in communications systems design is to create optimal source codes and message compressors by taking into account full knowledge of the message statistics and transmission channel capacity.  Information theory, which is a branch of applied mathematics and signal processing provides the necessary mathematical tools to find the fundamental limits on such signal processing operations.  It specifies a way of quantifying the amount of information that an observer possesses concerning a given phenomenon when only it’s probability density function (PDF) is known.  A key measure of information is entropy, which is defined as the amount of bits required to transmit one symbol in a message.  Transinformation, more commonly known as mutual information, measures the amount of information that can be obtained about one random variable by observing another.  Such information theoretic descriptors are of exceptional value because they abstract the structure of the PDF in an alternate, arguably more robust way than PDF qualifiers based on statistical moments (mean, variance, kurtosis etc).

So for what class of machine learning problems can information theoretic measures be acceptable descriptors of the data?  To answer this one much first be cognizant of the differences between information theory applications and machine learning applications. 

The fundamental problem in machine learning is rather different to that in communications system design.  Firstly, machine learning applications don’t necessarily assume parametric models of the PDF of the data that is to be learned.  There is no requirement for such a dependency because in many applications the data conform to non-gaussian PDFs that are possibly varying in time.  This observation suggests that the usefulness of information theory for machine learning applications is predicated upon non-parametric estimators of information theoretic measures.  

In supervised learning the goal is to estimate the joint PDF between the input and desired responses, whereas in unspervised learning the input data statistics are not only unknown but their quantification constitutes the goal of the analysis.    A priori knowledge required to apply information theory in the communications system design setting in fact turns out to be the main final objective in machine learning applications. 
 
Communications systems design require a priori knowledge, whereas machine learning is in part about estimating a priori knowledge.

This observations seem to suggest machine learning and information theory are orthogonal to each other and have not much useful in common.  Nonetheless information theory still has a lot to offer in the design of machine learning algorithms even when information theoretic metrics do not constitute the ultimate goal of the solution.

Primary niches for information theoretic applications in machine learning are to:

1. Develop alternate frameworks for the design of learning systems.
2. Foster improved cost functions for optimisation and learning algorithms.
3. Quantify the data’s statistical microstructure with better and more precise descriptors.

At a conceptual level, information theory can provide an alternate view and way of thinking because it focuses the designer’s attentions to the preservation and transfer of information between inputs and outputs of sub-systems.  For example:

* Entropy and mutual information quantify more precisely the data’s statistical microstructure when compared with second order statistical moments because they abstract the idea of information content.  

* The Reyni entropy, which through its special form decouples the logarithm from its argument, opens up new applications for optimal filtering regresion, clustering and classification - all currently based on the exploitation of the L2 norm of the data PDF as a cost function which are known to underperform when the data is non-gaussian.

Information theoretic learning provides a way of replacing statistical moments with a more robust geometric interpretation of data where the following substitutions occur:

| Statistical Learning Descriptors  | Information Theoretic Learning Descriptors  |
|:---:|:---:|
|  Variance | Entropy  |
|  Correlation | Correntropy |
|  Auto-correlation | Auto-correntropy |
|  Mean Squared Error | Minimum Entropy Error |
|  Distance in data space | Distance in probability space |

These facilitate the development of a new a set of mathematical tools with vastly different properties compared to conventional statistical based estimators that can be very useful in nonlinear, non-gaussian machine learning problems. This is already happening, for instance certain Deep Learning problems utilise a “cross-entropy” cost function for optimising neural networks. 

In future I shall explore in depth the intuition and conceptual understanding behind these and related information theoretic machine learning algorithms.


